{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82a79534-2257-48d7-b7de-73e2209f5b05",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f654d7-54ae-4e8f-b172-f7622c3a1e3f",
   "metadata": {},
   "source": [
    "In this NoteBook, I am just testing to see whether I can create `Ostrich`'s `TPL` files using `MESHFlow`'s Jinja2 templating engines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0bbc6f-dc04-40e3-88f7-5151f2e6b621",
   "metadata": {},
   "source": [
    "This may require some changes in the `Jinja2` templating files to be able to accept strings as values instead of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25f4b265-5566-446a-98b7-c420d2fa42df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import meshflow as mf\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "\n",
    "from typing import (\n",
    "    Dict,\n",
    "    List,\n",
    "    Sequence,\n",
    "    Union,\n",
    ")\n",
    "from pathlib import Path\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6524e6e-6800-4614-988e-058e1bd7a297",
   "metadata": {},
   "source": [
    "We just directly go ahead and call the templating functions. But before that, it is important to parse the `CLASS` files and build the three distinct dictionaries out of it for the templating functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1e8e0e9-e745-4d17-a5e6-abea370efc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_file = './wolf-creek-research-basin/mesh/MESH_parameters_CLASS.ini'\n",
    "hydrology_file = './wolf-creek-research-basin/mesh/MESH_parameters_hydrology.ini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1fcaa8b-3643-45af-81e4-76ba1a83e91b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def _remove_comments(string) -> str:\n",
    "    '''remove comment strings in the CLASS file strings'''\n",
    "    #return re.sub(r'\\s+\\d{2}\\s.*$', '', sections[0], flags=re.MULTILINE)\n",
    "    return re.sub(r'\\s+\\d{2}\\s(?:[^\\n ]| (?! ))*$', '', string, flags=re.MULTILINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb877091-65bf-4dc0-8ec9-68039200577f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# function to read a section in form of a pandas.DataFrame\n",
    "def class_section_divide(section: str, **read_csv_kwargs):\n",
    "    '''Refer to the following link for comprehensive, and hard-coded\n",
    "    values for the CLASS sections are implemented as there is no other\n",
    "    way around it.\n",
    "    '''\n",
    "    # split lines\n",
    "    lines = section.splitlines()\n",
    "\n",
    "    # build a dictionary out of CLASS sections\n",
    "    class_section = {}\n",
    "    \n",
    "    # vegetation parameters\n",
    "    class_section['veg1'] = \"\\n\".join(lines[:4])\n",
    "    class_section['veg2'] = \"\\n\".join(lines[4:7])\n",
    "    \n",
    "    # surface/hydraulic parameters\n",
    "    class_section['hyd1'] = lines[7]\n",
    "    class_section['hyd2'] = lines[8]\n",
    "\n",
    "    # soil parameters\n",
    "    class_section['soil'] = \"\\n\".join(lines[9:12])\n",
    "\n",
    "    # prognostic parameters\n",
    "    class_section['prog1'] = lines[12] if len(lines[12]) > 0 else \"\"\n",
    "    class_section['prog2'] = lines[13] if len(lines[13]) > 0 else \"\"\n",
    "    class_section['prog3'] = lines[14] if len(lines[14]) > 0 else \"\"\n",
    "\n",
    "    # return dictionary\n",
    "    return class_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d56b841-9daf-4cfa-821e-c41c932b7bec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _parse_class_meta_data(\n",
    "    case_section : str,\n",
    ") -> Dict:\n",
    "    \"\"\"Parse the CLASS file's meta-data section to extract\n",
    "    `info_entry` and `case_entry` dictionaries, necessary\n",
    "    to run MESHFlow's `meshflow.utility.render_class_template`\n",
    "    function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    case_section : str\n",
    "        The section of the CLASS file that contains the meta-data.\n",
    "        It should be a string containing the first four lines of the\n",
    "        CLASS file, which are:\n",
    "        - Title\n",
    "        - Author\n",
    "        - Place\n",
    "        - Case information (centroid latitude, longitude, reference heights, etc.)\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    info_entry : Dict\n",
    "        A dictionary containing the author and location information.\n",
    "    case_entry : Dict\n",
    "        A dictionary containing the case information, including:\n",
    "        - Centroid latitude and longitude\n",
    "        - Reference heights for wind speed, specific humidity, and air temperature\n",
    "        - Reference height surface roughness\n",
    "        - Number of land cover types (NL)\n",
    "        - Number of soil types (NM)\n",
    "    \"\"\"\n",
    "    # remove comments from the section\n",
    "    case_section = _remove_comments(case_section)\n",
    "    \n",
    "    # hard-coded values based on different lines of the CLASS file\n",
    "    # the indices refer to line numbers in the section\n",
    "    title_line = case_section.splitlines()[0]\n",
    "    author_line = case_section.splitlines()[1]\n",
    "    place_line = case_section.splitlines()[2]\n",
    "    case_line = case_section.splitlines()[3]\n",
    "\n",
    "    # now building dictionaries that MESHFlow needs just here for\n",
    "    # simplicity\n",
    "    info_entry = {\n",
    "        \"author\": author_line.strip(),\n",
    "        \"location\": place_line.strip(),\n",
    "    }\n",
    "\n",
    "    # now building the `case_entry` containing extra meta-data information\n",
    "    # about the data\n",
    "\n",
    "    # first stripping and splitting the `case_line` string\n",
    "    case_line = case_line.strip().split()\n",
    "    # build `case_entry` key-value pairs, note that the keys are hard-coded\n",
    "    # to match `MESHFlow`'s requirements\n",
    "    case_entry = {\n",
    "        \"centroid_lat\": float(case_line[0]), # float value\n",
    "        \"centroid_lon\": float(case_line[1]), # float value\n",
    "        \"reference_height_wndspd\": float(case_line[2]), # float value\n",
    "        \"reference_height_spechum_airtemp\": float(case_line[3]), # float value\n",
    "        \"reference_height_surface_roughness\": float(case_line[4]), # float value\n",
    "        \"NL\": int(case_line[-2]), # integer value, number of sub-basins\n",
    "        \"NM\": int(case_line[-1]), # integer value, number of GRU blocks\n",
    "    }\n",
    "    \n",
    "    return info_entry, case_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4983783d-8895-472f-b8ee-48f4309498d3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def _determine_gru_type(\n",
    "    line : str\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Return the 1-based column index of the first numeric token that is exactly '1.000'\n",
    "    (or numerically equal to 1.0 with three decimal places) in a line of mixed data.\n",
    "\n",
    "    The line may contain:\n",
    "      - Multiple spaces between columns\n",
    "      - Trailing non-numeric descriptor fields (tokens containing any letter)\n",
    "      - Other numeric fields (including things like 05)\n",
    "\n",
    "    Parsing stops once a token containing any alphabetic character appears, assuming\n",
    "    the remainder are descriptors rather than data columns.\n",
    "\n",
    "    Args:\n",
    "        line: A string containing whitespace-separated columns.\n",
    "\n",
    "    Returns:\n",
    "        The 1-based column number where the first 1.000 occurs, or None if not found.\n",
    "    \"\"\"\n",
    "    tokens = line.strip().split()\n",
    "    slice_len = min(5, len(tokens))\n",
    "    \n",
    "    # to track mixed GRU types and also \n",
    "    gru_type_sum = 0\n",
    "    \n",
    "    # iterate over the first line of the vegetation parameter section\n",
    "    for i in range(slice_len):\n",
    "        # if a distinct GRU, look for 1.000 value\n",
    "        if tokens[i] == \"1.000\":\n",
    "            return i + 1  # 1-based\n",
    "\n",
    "        # Calculate the sum until this for loop breaks\n",
    "        # or ends\n",
    "        gru_type_sum += float(tokens[i])\n",
    "\n",
    "    # FIXME: if sum equals to 1, then that means we deal with a mixed GRU\n",
    "    #        type, and we will have to add the relevant feature to both\n",
    "    #        MESHFlow and MESHFIAT;\n",
    "    #        For now, find the first column without non-zero value\n",
    "    if gru_type_sum == 1:\n",
    "        for i in range(slice_len):\n",
    "            if float(tokens[i]) > 0:\n",
    "                return i + 1\n",
    "\n",
    "    # Raise an error if it is not a valid CLASS field\n",
    "    if gru_type_sum == 0:\n",
    "        raise ValueError(\"Invalid CLASS GRU type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0c764e7-3dfa-4b8b-9f8c-5af772f20774",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def _parse_class_veg1(\n",
    "    veg_section : str,\n",
    "    gru_idx : int,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # the `veg_section` must only be 4 lines\n",
    "    veg_lines = veg_section.splitlines()\n",
    "    \n",
    "    if len(veg_lines) != 4:\n",
    "        raise ValueError(\"The vegetation section must have exactly 4 lines.\")\n",
    "\n",
    "    # gru index is the 1-based index of the GRU type\n",
    "    # so the index of the first column is gru_idx - 1\n",
    "    # for the 5th type, the second section of each block\n",
    "    # will have a value of `0`\n",
    "    idx = gru_idx - 1\n",
    "\n",
    "    # please note that the parameters are hard-coded and match the inputs\n",
    "    # of MESHFlow's `meshflow.utility.render_class_template` function.\n",
    "    if 1 <= gru_idx <= 4: # non-barren-land types\n",
    "        veg_params = {\n",
    "            # first-line parameters of the block\n",
    "            'fcan': float(veg_lines[0].strip().split()[idx]),\n",
    "            'lamx': float(veg_lines[0].strip().split()[idx + 5]),\n",
    "            # second-line parameters\n",
    "            'lnz0': float(veg_lines[1].strip().split()[idx]),\n",
    "            'lamn': float(veg_lines[1].strip().split()[idx + 5]),\n",
    "            # third-line parameters\n",
    "            'alvc': float(veg_lines[2].strip().split()[idx]),\n",
    "            'cmas': float(veg_lines[2].strip().split()[idx + 5]),\n",
    "            # fourth-line parameters\n",
    "            'alic': float(veg_lines[3].strip().split()[idx]),\n",
    "            'root': float(veg_lines[3].strip().split()[idx + 5]),\n",
    "        }\n",
    "\n",
    "    elif gru_idx == 5:\n",
    "        veg_params = {\n",
    "            # first-line parameters of the block\n",
    "            'fcan': float(veg_lines[0].strip().split()[idx]),\n",
    "            'lamx': 0.0,\n",
    "            # second-line parameters\n",
    "            'lnz0': float(veg_lines[1].strip().split()[idx]),\n",
    "            'lamn': 0.0,\n",
    "            # third-line parameters\n",
    "            'alvc': float(veg_lines[2].strip().split()[idx]),\n",
    "            'cmas': 0.0,\n",
    "            # fourth-line parameters\n",
    "            'alic': float(veg_lines[3].strip().split()[idx]),\n",
    "            'root': 0.0,\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid GRU index. Must be between 1 and 5.\")\n",
    "\n",
    "    return veg_params\n",
    "\n",
    "def _parse_class_veg2(\n",
    "    veg_section : str,\n",
    "    gru_idx : int,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # the `veg_section` must only be 3 lines\n",
    "    veg_lines = veg_section.splitlines()\n",
    "    \n",
    "    if len(veg_lines) != 3:\n",
    "        raise ValueError(\"The vegetation section must have exactly 4 lines.\")\n",
    "\n",
    "    # gru index is the 1-based index of the GRU type\n",
    "    # so the index of the first column is gru_idx - 1\n",
    "    # for the 5th type, the second section of each block\n",
    "    # will have a value of `0`\n",
    "    idx = gru_idx - 1\n",
    "\n",
    "    # please note that the parameters are hard-coded and match the inputs\n",
    "    # of MESHFlow's `meshflow.utility.render_class_template` function.\n",
    "    if 1 <= gru_idx <= 4: # non-barren-land types\n",
    "        veg_params = {\n",
    "            # first-line parameters of the block\n",
    "            'rsmn': float(veg_lines[0].strip().split()[idx]),\n",
    "            'qa50': float(veg_lines[0].strip().split()[idx + 5]),\n",
    "            # second-line parameters\n",
    "            'vpda': float(veg_lines[1].strip().split()[idx]),\n",
    "            'vpdb': float(veg_lines[1].strip().split()[idx + 5]),\n",
    "            # third-line parameters\n",
    "            'psga': float(veg_lines[2].strip().split()[idx]),\n",
    "            'psgb': float(veg_lines[2].strip().split()[idx + 5]),\n",
    "        }\n",
    "\n",
    "    elif gru_idx == 5:\n",
    "        param_names = ['rsmn', 'qa50', 'vpda', 'vpdb', 'psga', 'psgb']\n",
    "        veg_params = {k: 0.0 for k in param_names}\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid GRU index. Must be between 1 and 5.\")\n",
    "\n",
    "    return veg_params\n",
    "\n",
    "def _parse_class_hyd1(\n",
    "    hyd_line : str,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # remove comments\n",
    "    hyd_line = _remove_comments(hyd_line)\n",
    "\n",
    "    # strip and split based on whitespace\n",
    "    hyd_line = hyd_line.strip().split()\n",
    "\n",
    "    # please note that the parameters are hard-coded and match the inputs\n",
    "    # of MESHFlow's `meshflow.utility.render_class_template` function.\n",
    "    veg_params = {\n",
    "        'drn': float(hyd_line[0]),\n",
    "        'sdep': float(hyd_line[1]),\n",
    "        'fare': float(hyd_line[2]),\n",
    "        'dd': float(hyd_line[3]),\n",
    "    }\n",
    "\n",
    "    return veg_params\n",
    "\n",
    "def _parse_class_hyd2(\n",
    "    hyd_line : str,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # remove comments\n",
    "    hyd_line = _remove_comments(hyd_line)\n",
    "\n",
    "    # strip and split based on whitespace\n",
    "    hyd_line = hyd_line.strip().split()\n",
    "\n",
    "    # please note that the parameters are hard-coded and match the inputs\n",
    "    # of MESHFlow's `meshflow.utility.render_class_template` function.\n",
    "    hyd_params = {\n",
    "        'xslp': float(hyd_line[0]),\n",
    "        'xdrainh': float(hyd_line[1]),\n",
    "        'mann': float(hyd_line[2]),\n",
    "        'ksat': float(hyd_line[3]),\n",
    "        'mid': \" \".join(hyd_line[5:])\n",
    "    }\n",
    "\n",
    "    return hyd_params\n",
    "\n",
    "def _parse_class_soil(\n",
    "    soil_section : str,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    # remove comments\n",
    "    soil_section = _remove_comments(soil_section)\n",
    "\n",
    "    # strip and split based on whitespace\n",
    "    soil_lines = soil_section.splitlines()\n",
    "\n",
    "    # please note that the parameters are hard-coded and match the inputs\n",
    "    # of MESHFlow's `meshflow.utility.render_class_template` function.\n",
    "    soil_params = {\n",
    "        # first line parameters\n",
    "        'sand1': float(soil_lines[0].strip().split()[0]),\n",
    "        'sand2': float(soil_lines[0].strip().split()[1]),\n",
    "        'sand3': float(soil_lines[0].strip().split()[2]),\n",
    "        # second line parameters\n",
    "        'clay1': float(soil_lines[1].strip().split()[0]),\n",
    "        'clay2': float(soil_lines[1].strip().split()[1]),\n",
    "        'clay3': float(soil_lines[1].strip().split()[2]),\n",
    "        # third line parameters\n",
    "        'orgm1': float(soil_lines[2].strip().split()[0]),\n",
    "        'orgm2': float(soil_lines[2].strip().split()[1]),\n",
    "        'orgm3': float(soil_lines[2].strip().split()[2]),\n",
    "    }\n",
    "\n",
    "    return soil_params\n",
    "\n",
    "def _parse_class_prog1(\n",
    "    prog_line : str,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # remove comments\n",
    "    prog_line = _remove_comments(prog_line)\n",
    "\n",
    "    # strip and split based on whitespace\n",
    "    prog_line = prog_line.strip().split()\n",
    "\n",
    "    # please note that the parameters are hard-coded and match the inputs\n",
    "    # of MESHFlow's `meshflow.utility.render_class_template` function.\n",
    "    prog_params = {\n",
    "        'tbar1': float(prog_line[0]),\n",
    "        'tbar2': float(prog_line[1]),\n",
    "        'tbar3': float(prog_line[2]),\n",
    "        'tcan': float(prog_line[3]),\n",
    "        'tsno': float(prog_line[4]),\n",
    "        'tpnd': float(prog_line[5]),\n",
    "    }\n",
    "\n",
    "    return prog_params\n",
    "\n",
    "def _parse_class_prog2(\n",
    "    prog_line : str,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # remove comments\n",
    "    prog_line = _remove_comments(prog_line)\n",
    "\n",
    "    # strip and split based on whitespace\n",
    "    prog_line = prog_line.strip().split()\n",
    "\n",
    "    # please note that the parameters are hard-coded and match the inputs\n",
    "    # of MESHFlow's `meshflow.utility.render_class_template` function.\n",
    "    prog_params = {\n",
    "        'thlq1': float(prog_line[0]),\n",
    "        'thlq2': float(prog_line[1]),\n",
    "        'thlq3': float(prog_line[2]),\n",
    "        'thic1': float(prog_line[3]),\n",
    "        'thic2': float(prog_line[4]),\n",
    "        'thic3': float(prog_line[5]),\n",
    "        'zpnd': float(prog_line[6]),\n",
    "    }\n",
    "\n",
    "    return prog_params\n",
    "\n",
    "def _parse_class_prog3(\n",
    "    prog_line : str,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # remove comments\n",
    "    prog_line = _remove_comments(prog_line)\n",
    "\n",
    "    # strip and split based on whitespace\n",
    "    prog_line = prog_line.strip().split()\n",
    "\n",
    "    # please note that the parameters are hard-coded and match the inputs\n",
    "    # of MESHFlow's `meshflow.utility.render_class_template` function.\n",
    "    prog_params = {\n",
    "        'rcan': float(prog_line[0]),\n",
    "        'scan': float(prog_line[1]),\n",
    "        'sno': float(prog_line[2]),\n",
    "        'albs': float(prog_line[3]),\n",
    "        'rhos': float(prog_line[4]),\n",
    "        'gro': float(prog_line[5])\n",
    "    }\n",
    "\n",
    "    return prog_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f4bf276-b866-4bbf-99c4-016e8ba24b50",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def _analyze_class_file(\n",
    "    class_file: Union[os.PathLike, str],\n",
    ") -> Dict[str, Union[Dict, str]]:\n",
    "    \"\"\"\n",
    "    Analyze the CLASS file and return a dictionary containing the parsed sections.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    class_file : Union[PathLike | str]\n",
    "        The path to the CLASS file to be analyzed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[Dict[str, Union[Dict, str]]]\n",
    "        A dictionary containing the parsed sections of the CLASS file.\n",
    "    \"\"\"\n",
    "    # read the text file\n",
    "    text = Path(class_file).read_text(encoding=\"utf-8\")\n",
    "    \n",
    "    # Split where there is at least one completely blank line (possibly with spaces)\n",
    "    sections = re.split(r'\\r?\\n\\s*\\r?\\n', text.strip())\n",
    "    \n",
    "    # first section is typically the information section\n",
    "    # the middle sections are CLASS computational unit blocks, each\n",
    "    #     containing vegetation, soil, hydrology, and prognostic parameters\n",
    "    # the last section are the dates that should not be processed and \n",
    "    #     its content does not matter for the analysis\n",
    "\n",
    "    # building dictionaries out of the first section needed for \n",
    "    # MESHFLOW's `meshflow.utility.render_class_template` function\n",
    "    info_entry, case_entry = \\\n",
    "        _parse_class_meta_data(sections[0])\n",
    "\n",
    "    # create an empty gru_entry dictionary to be further\n",
    "    # populated by the following iterative loop\n",
    "    gru_entry = {}\n",
    "\n",
    "    # iterating over the sections until the last one\n",
    "    for idx, section in enumerate(sections[1:-1], start=1):\n",
    "        # divide the section into a dictionary of sections\n",
    "        class_section = class_section_divide(section=section)\n",
    "\n",
    "        # determine GRU type, based on CLASS assumptions:\n",
    "        #    1. needleleaf forest\n",
    "        #    2. broadleaf forest\n",
    "        #    3. cropland\n",
    "        #    4. grassland\n",
    "        #    5. urban, barren land, or imprevious area\n",
    "        section_landcover_type = _determine_gru_type(\n",
    "            line=class_section['veg1'].splitlines()[0],\n",
    "        )\n",
    "        # based on the number extracted above, we can name the\n",
    "        # GRU class\n",
    "        class_name_dict = {\n",
    "            1: \"needleleaf\",\n",
    "            2: \"broadleaf\",\n",
    "            3: \"crop\",\n",
    "            4: \"grassland\",\n",
    "            5: \"urban\",\n",
    "        }\n",
    "\n",
    "        # parse the sections -- hard-coded as there are no\n",
    "        # other alternatives\n",
    "        veg1_params = _parse_class_veg1(\n",
    "            veg_section=class_section['veg1'],\n",
    "            gru_idx=section_landcover_type,\n",
    "        )\n",
    "        veg2_params = _parse_class_veg2(\n",
    "            veg_section=class_section['veg2'],\n",
    "            gru_idx=section_landcover_type,\n",
    "        )\n",
    "        hyd1_params = _parse_class_hyd1(\n",
    "            hyd_line=class_section['hyd1'],\n",
    "        )\n",
    "        hyd2_params = _parse_class_hyd2(\n",
    "            hyd_line=class_section['hyd2'],\n",
    "        )\n",
    "        soil_params = _parse_class_soil(\n",
    "            soil_section=class_section['soil'],\n",
    "        )\n",
    "        prog1_params = _parse_class_prog1(\n",
    "            prog_line=class_section['prog1'],\n",
    "        )\n",
    "        prog2_params = _parse_class_prog2(\n",
    "            prog_line=class_section['prog2'],\n",
    "        )\n",
    "        prog3_params = _parse_class_prog3(\n",
    "            prog_line=class_section['prog3'],\n",
    "        )\n",
    "\n",
    "        # make a list of parameters for easier literal unpacking inside\n",
    "        # the gru_entry dictionary\n",
    "        param_list = [\n",
    "            veg1_params,\n",
    "            veg2_params,\n",
    "            hyd1_params,\n",
    "            hyd2_params,\n",
    "            soil_params,\n",
    "            prog1_params,\n",
    "            prog2_params,\n",
    "            prog3_params,\n",
    "        ]\n",
    "\n",
    "        # make sure to make an exception for water-like land covers\n",
    "        if 'water' in hyd2_params['mid'].lower():\n",
    "            class_type = 'water'\n",
    "        elif 'snow' in hyd2_params['mid'].lower():\n",
    "            class_type = 'water'\n",
    "        elif 'ice' in hyd2_params['mid'].lower():\n",
    "            class_type = 'water'\n",
    "        else:\n",
    "            class_type = class_name_dict[section_landcover_type]\n",
    "\n",
    "        # adding class type info\n",
    "        gru_entry[idx] = {\n",
    "            'class': class_type,\n",
    "        }\n",
    "        # adding parameters\n",
    "        gru_entry[idx].update({k: v for d in param_list for k, v in d.items()})\n",
    "\n",
    "    return case_entry, info_entry, gru_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4774d837-f39d-444b-a427-09092e17a914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "case_entry, info_entry, gru_entry = _analyze_class_file(class_file=class_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da86fd3-2412-444c-8b20-f41658462eec",
   "metadata": {},
   "source": [
    "Now that we have the necessary dictionary, we can reproduce the CLASS file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a99373ce-0ba5-4b49-a08d-7bf433f72525",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "c = mf.utility.render_class_template(\n",
    "    class_case=case_entry,\n",
    "    class_info=info_entry,\n",
    "    class_grus=gru_entry,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e2bea3-5621-4cd8-b60b-3b27004a1ed6",
   "metadata": {},
   "source": [
    "Let's also analyze the `hydrology` file and build that as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cdf8f67-d03e-42bb-aec7-7d49d168782f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def iter_sections(\n",
    "    text: str,\n",
    "    drop_separators: bool=True,\n",
    "):\n",
    "    # default re directives\n",
    "    HEADER_RE = re.compile(r'^#{3,}\\s*(.*?)\\s*#*\\s*$', re.MULTILINE)\n",
    "    SEP_LINE_RE = re.compile(r'^-{3,}#.*$')\n",
    "\n",
    "    # defining matching headers\n",
    "    matches = list(HEADER_RE.finditer(text))\n",
    "    def not_sep(line):\n",
    "        return not (drop_separators and SEP_LINE_RE.match(line))\n",
    "\n",
    "    if not matches:\n",
    "        body = \"\\n\".join(l for l in text.splitlines() if not_sep(l)).strip()\n",
    "        if body:\n",
    "            yield (\"Preamble\", body)\n",
    "        return\n",
    "\n",
    "    # preamble\n",
    "    first_start = matches[0].start()\n",
    "    if first_start > 0:\n",
    "        pre_lines = [l for l in text[:first_start].splitlines() if not_sep(l)]\n",
    "        pre = \"\\n\".join(pre_lines).strip()\n",
    "        if pre:\n",
    "            yield (\"Preamble\", pre)\n",
    "\n",
    "    # extracting sections\n",
    "    for i, m in enumerate(matches):\n",
    "        header = m.group(1).strip()\n",
    "        body_start = m.end()\n",
    "        body_end = matches[i+1].start() if i+1 < len(matches) else len(text)\n",
    "        block = text[body_start:body_end]\n",
    "        lines = [l for l in block.splitlines() if not_sep(l)]\n",
    "        body = \"\\n\".join(lines).strip('\\n')\n",
    "        yield (header, body)\n",
    "\n",
    "def hydrology_section_divide(\n",
    "    hydrology_file: os.PathLike | str,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    text = Path(hydrology_file).read_text(encoding=\"utf-8\")\n",
    "    sections = [b for h, b in iter_sections(text)]\n",
    "\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1ee4296-6fde-4683-b060-cc38f96feb8c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def _analyze_hydrology(\n",
    "    hydrology_file : Union[os.PathLike, str],\n",
    ") -> Dict[str, Union[int, float]]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # extract sections from the hydrology file\n",
    "    sections = hydrology_section_divide(hydrology_file)\n",
    "    \n",
    "    # first, the routing dictionary\n",
    "    routing_df = pd.read_csv(StringIO(sections[2]), comment='#', sep='\\s+', index_col=0, skiprows=1, header=None)\n",
    "    routing_df.index = routing_df.index.str.lower()\n",
    "    # we should return a list of values\n",
    "    routing_dict = [v for v in routing_df.to_dict().values()]\n",
    "\n",
    "    # and second, the hydrology dictionary\n",
    "    hydrology_df = pd.read_csv(StringIO(sections[4]), comment='#', sep='\\s+', index_col=0, skiprows=2, header=None)\n",
    "    hydrology_df.index = hydrology_df.index.str.lower()\n",
    "    # and we return a dictionary of this\n",
    "    hydrology_dict = hydrology_df.to_dict()\n",
    "    \n",
    "    return routing_dict, hydrology_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "007313f5-4533-4edc-8505-29f6ff90f41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "routing_dict, hydrology_dict = _analyze_hydrology(hydrology_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b0a7a25-2e84-4ebe-b5d3-afc614bdcccf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "d = mf.utility.render_hydrology_template(\n",
    "    routing_params=routing_dict,\n",
    "    hydrology_params=hydrology_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130f663c-9b03-4b0d-a537-537ae54a7039",
   "metadata": {},
   "source": [
    "Here, we try to make template files that `Ostrich` can read, as an example. The choice of `Ostrich` is just an example, and can be anything else. A friendly reminder to NOT hyperventilate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219ab962-0a81-4384-88f2-ba74b466be8b",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68524384-578f-4b34-9751-a70255d747f1",
   "metadata": {},
   "source": [
    "Let's focus on the first dictionary, `gru_entry` describing `CLASS` parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fede8ff-31a2-495d-87b3-2dd93dabd06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom typehints\n",
    "NameType = Union[str, int, float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44aa34ca-ef51-4d52-92db-44d4cc629698",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def param_name_gen(\n",
    "    computational_unit: NameType,\n",
    "    name: NameType,\n",
    ") -> str:\n",
    "    \"\"\"Generalized method to template parameter names\n",
    "    based on hydrological computational unit (gru, hru,\n",
    "    etc.) and the name of the parameter\n",
    "    \"\"\"\n",
    "    # making strings\n",
    "    _unit = str(computational_unit)\n",
    "    _name = str(name)\n",
    "\n",
    "    # A naming template like the following can be\n",
    "    # generalized to all models: _+`_unit`+`_name`\n",
    "    param_name = '_' + _unit.upper() + _name.upper()\n",
    "    \n",
    "    return param_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4656f04-e115-47f1-95cc-6eaf0effdd80",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def param_list_gen(\n",
    "    names : Dict[NameType, Sequence[NameType]],\n",
    ") -> Dict[NameType, Dict[NameType, NameType]]:\n",
    "    \"\"\"Creating template names for the collection of parameters\n",
    "    entering calibration experiments.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    names : dict\n",
    "        A dictionary of names with keys set as computational unit of a\n",
    "        model of interest, and the values being the sequence of model\n",
    "        names to enter the calibration process.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A similar dictionary to `names` but also providing target\n",
    "        template names for each parameters.\n",
    "    \"\"\"\n",
    "    # creating an empty dictionary to hold the template names\n",
    "    template_names = {}\n",
    "    \n",
    "    # iterating over the names keys\n",
    "    for unit in names.keys():\n",
    "        # creating names for each parameter value in the sequence\n",
    "        template_names[unit] = {\n",
    "            name: param_name_gen(unit, name) for name in names[unit]\n",
    "        }\n",
    "\n",
    "    return template_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37363940-e56e-4f6a-ba14-354681720110",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    1: ['sno', 'rhos', 'clay1', 'sand1', 'clay3'],\n",
    "    3: ['lamx', 'cmas', 'clay1', 'clay2'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3ccb2c5-62de-452f-92e5-bd9302e59ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'sno': '_1SNO',\n",
       "  'rhos': '_1RHOS',\n",
       "  'clay1': '_1CLAY1',\n",
       "  'sand1': '_1SAND1',\n",
       "  'clay3': '_1CLAY3'},\n",
       " 3: {'lamx': '_3LAMX',\n",
       "  'cmas': '_3CMAS',\n",
       "  'clay1': '_3CLAY1',\n",
       "  'clay2': '_3CLAY2'}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_params = param_list_gen(params)\n",
    "template_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd136b0-a96b-42a6-851e-8df2c5207121",
   "metadata": {},
   "source": [
    "Now that I have new names for parameters of certain computational units (in MESH is GRU -- don't hyperventilate MC), it's time to update the parameter values and make template files for `OSTRICH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4524d8bb-8376-43bb-ae4d-5c15767fe071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the computational units\n",
    "for unit in template_params.keys():\n",
    "    # update the values of parameters in each unit\n",
    "    unit_params = template_params[unit]\n",
    "    for p in unit_params.keys():\n",
    "        if p in gru_entry[unit].keys():\n",
    "            # updating the target gru_entry dictionary\n",
    "            gru_entry[unit][p] = unit_params[p]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41f901b-0f5a-401b-90e2-548d4dd5d004",
   "metadata": {},
   "source": [
    "Let's dump this into a JSON file and see how it looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b5f1552-cea3-433b-8824-f5cc5a3e4e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the directory if doesn't exist\n",
    "os.makedirs('./junk_files', exist_ok=True)\n",
    "# dump the data as a JSON file\n",
    "with open('./junk_files/class.json', 'w') as f:\n",
    "    json.dump(gru_entry, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0ee632-0f48-4cbb-85e8-703e34050dd3",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66d029d-cea1-4e55-bbc0-eeb83abc2250",
   "metadata": {},
   "source": [
    "Now, we have manually changed the `class_changed.json` file, let's read it back and see if we can decode it properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d7be4e5-f8ab-43dd-a223-91486287037f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Precompile regexes for speed/readability\n",
    "_INT_RE = re.compile(r'^[-+]?\\d+$')\n",
    "_FLOAT_RE = re.compile(\n",
    "    r\"\"\"^[-+]?(                # optional sign\n",
    "        (?:\\d+\\.\\d*|\\d*\\.\\d+)  # something with a decimal point\n",
    "        (?:[eE][-+]?\\d+)?      # optional exponent\n",
    "        |\n",
    "        \\d+[eE][-+]?\\d+        # or integer with exponent (e.g. 1e6)\n",
    "    )$\"\"\",\n",
    "    re.X\n",
    ")\n",
    "\n",
    "def parse_numeric_string(s: str):\n",
    "    \"\"\"\n",
    "    Try to interpret a numeric-looking string as int or float.\n",
    "    Return the converted number, or the original string if not numeric.\n",
    "    \"\"\"\n",
    "    if _INT_RE.match(s):\n",
    "        # Keep as int if it fits typical Python int (Python int is unbounded anyway)\n",
    "        return int(s)\n",
    "    if _FLOAT_RE.match(s):\n",
    "        # Anything with decimal point or exponent\n",
    "        return float(s)\n",
    "    return s  # not numeric-looking\n",
    "\n",
    "def convert_numeric_strings(obj):\n",
    "    \"\"\"\n",
    "    Recursively walk lists/dicts and convert numeric-like strings.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_numeric_strings(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, list):\n",
    "        return [convert_numeric_strings(v) for v in obj]\n",
    "    if isinstance(obj, str):\n",
    "        return parse_numeric_string(obj.strip())\n",
    "    return obj  # leaves int, float, bool, None, etc. untouched\n",
    "\n",
    "\n",
    "def make_object_hook():\n",
    "    def object_hook(d):\n",
    "        for k, v in d.items():\n",
    "            d[k] = convert_numeric_strings(v)  # reuse earlier function\n",
    "        return d\n",
    "    return object_hook\n",
    "\n",
    "with open('./junk_files/class_changed.json', \"r\", encoding=\"utf-8\") as f:\n",
    "    gru_entry_modified = json.load(f, object_hook=make_object_hook())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcd3b0cf-52bd-4155-bad8-a453b7d3e043",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c_modified = mf.utility.render_class_template(\n",
    "    class_case=case_entry,\n",
    "    class_info=info_entry,\n",
    "    class_grus=gru_entry_modified,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac35be0-e68f-44f6-83d3-a80e1cec531f",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546ccb83-ac65-4001-a569-9d559f50f79c",
   "metadata": {},
   "source": [
    "Now, we have to make a Jinja2 template of the parameters, and their ranges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1ac36da-918b-4fff-ad41-f83557b8b344",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# third-party libraries\n",
    "from jinja2 import (\n",
    "    Environment,\n",
    "    FileSystemLoader,\n",
    "    PackageLoader,\n",
    ")\n",
    "\n",
    "# built-in libraries\n",
    "import os\n",
    "import json\n",
    "import copy\n",
    "import time\n",
    "\n",
    "# global variables and helper functions\n",
    "def raise_helper(msg):\n",
    "    \"\"\"Jinja2 helper function to raise exceptions.\"\"\"\n",
    "    raise Exception(msg)\n",
    "\n",
    "# Jinja2 environment setup\n",
    "environment = Environment(\n",
    "    # loader=PackageLoader(\"meshflow\", \"templates\"),\n",
    "    loader=FileSystemLoader('../src/fiatmodel/calibration/ostrich/templates/'),\n",
    "    trim_blocks=True,\n",
    "    lstrip_blocks=True,\n",
    "    line_comment_prefix='##',\n",
    ")\n",
    "\n",
    "environment.globals['raise'] = raise_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2afd2547-f9aa-4f85-b10b-fe16fd735adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_entry_bounds = {\n",
    "    1: {\n",
    "        'sno': [0.0, 0.5],\n",
    "        'rhos': [0.0, 0.5],\n",
    "        'clay1': [10, 50],\n",
    "        'sand1': [30, 70],\n",
    "        'clay3': [20, 30],\n",
    "    },\n",
    "    3: {\n",
    "        'lamx': [0.0, 1.0],\n",
    "        'cmas': [0.0, 1.0],\n",
    "        'clay2': [20, 60],\n",
    "        'clay1': [0, 10],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65042242-9601-42e2-ba08-0e43dc973b3c",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f530536-a314-4fcb-b8ac-660c0a3b7fd7",
   "metadata": {},
   "source": [
    "Now, working on constraints, notice that this is a `MESH` specific workflow, and the generalized form from this example should be applicable to other models as well, and calibration parts should be able to interpret this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "018d258e-fe21-4adb-9f82-4a312f28f9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'sno': [0.0, 0.5],\n",
       "  'rhos': [0.0, 0.5],\n",
       "  'clay1': [10, 50],\n",
       "  'sand1': [30, 70],\n",
       "  'clay3': [20, 30]},\n",
       " 3: {'lamx': [0.0, 1.0],\n",
       "  'cmas': [0.0, 1.0],\n",
       "  'clay2': [20, 60],\n",
       "  'clay1': [0, 10]}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_entry_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4fd4c892-ddf0-4143-8879-209fc2ec5fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a list of parameters that need to be included in contraints\n",
    "# these are MESH-specific\n",
    "constraints_params_template = ['clay', 'sand']\n",
    "# and building invidiual parameters present in all MESH configurations\n",
    "constraint_params = []\n",
    "\n",
    "# default is assuming MESH has 3 soil layers\n",
    "for i in range(1, 4):\n",
    "    # iterate over the parameter template values\n",
    "    for p in constraints_params_template:\n",
    "        # create the parameter name\n",
    "        param_name = f\"{p.lower()}{i}\"\n",
    "        # append to the list\n",
    "        constraint_params.append(param_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3de8913-f994-48e8-a83f-921c15ef3efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clay1', 'sand1', 'clay2', 'sand2', 'clay3', 'sand3']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraint_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83618617-9b7c-48d6-8bc3-d95cdbcb8d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'sno': [0.0, 0.5],\n",
       "  'rhos': [0.0, 0.5],\n",
       "  'clay1': [10, 50],\n",
       "  'sand1': [30, 70],\n",
       "  'clay3': [20, 30]},\n",
       " 3: {'lamx': [0.0, 1.0],\n",
       "  'cmas': [0.0, 1.0],\n",
       "  'clay2': [20, 60],\n",
       "  'clay1': [0, 10]}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_entry_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d11be6c-bc10-426d-98e5-b7f10afc7459",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# iterate over `gru_entry`; the reason to iterate over `gru_entry` elements will be investigated later\n",
    "# as there are multiple dictionary to go over, so this needs to be logically detected and targetted.\n",
    "\n",
    "# calibration constraints for each computation unit\n",
    "calibration_constraints = {}\n",
    "\n",
    "for unit in gru_entry_bounds.keys():\n",
    "    # creating a set of parameters for the computational\n",
    "    # unit to be calibrated\n",
    "    calibrated_set = set(gru_entry_bounds[unit].keys())\n",
    "    \n",
    "    # check whether any of `constrain_params` elements are available\n",
    "    # in each computational unit's set of parameters\n",
    "    match = [x for _, x in enumerate(constraint_params) if x in calibrated_set]\n",
    "    \n",
    "    # set it aside if match is found\n",
    "    if match is not None:\n",
    "        calibration_constraints[unit] = match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ea74f2c-966a-456e-83a4-3335b782fbe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: ['clay1', 'sand1', 'clay3'], 3: ['clay1', 'clay2']}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calibration_constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90db414-683b-4fe4-9702-6b3b63e80a8b",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1304862c-5c3b-4521-ab44-309ce78d5c0d",
   "metadata": {},
   "source": [
    "There are two types of dictionaries that we need to deal with during the calibration process:\n",
    "1. `parameters` dictionary and\n",
    "2. `calibration_bounds` dictionary.\n",
    "\n",
    "All the elements in `calibration_bounds` necessarily need to be present in `parameters` dictionary.\n",
    "\n",
    "In this example, I just focus on the `parameters:gru_entry` and `calibration_bounds:gru_entry_bounds` dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f53fe26-66c1-49cc-a5ad-596838af9e48",
   "metadata": {},
   "source": [
    "To derive the contstraints for soil parameters, we have to have a vivid idea of `clay`, `sand` and `silt` values. In `MESH`, only `clay` and `sand` values are explicitely defined, and `silt = 100 - clay - sand` values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f364ff-e266-42ab-8f89-1fbffe1453ec",
   "metadata": {},
   "source": [
    "Therefore, constraints are defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229e15fb-884f-44c0-b09d-3c769aa229c4",
   "metadata": {},
   "source": [
    "`clay_x = (_XCLY_ / _XSUM_SOIL_)`\n",
    "\n",
    "and \n",
    "\n",
    "`_XSUM_SOIL_ = _XCLY_ + _XSND_ + _XSLT_`\n",
    "\n",
    "And all values are in percentage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e3dccc-07d1-43a1-b307-8dea13fa03e3",
   "metadata": {},
   "source": [
    "Note that, one or two soil parameters can be calibrated, for each soil layer. There is no limitation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08560d44-0803-4c82-b023-e2df99483531",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66b3bb47-8ceb-4a30-9dd2-aa424862cae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining bounds for various parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0617eeca-5f56-461b-be1f-56c92f88be93",
   "metadata": {},
   "source": [
    "## Hydrology parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18877655-048b-4149-8a12-c41115e91e9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hydro_params = {\n",
    "    1: ['zsnl', 'zplg'],\n",
    "    12: ['zpls', 'zsnl', 'zplg'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd2c8b5f-eb00-4a99-8e64-0881ab44da57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'zsnl': '_1ZSNL', 'zplg': '_1ZPLG'},\n",
       " 12: {'zpls': '_12ZPLS', 'zsnl': '_12ZSNL', 'zplg': '_12ZPLG'}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_hydro_params = param_list_gen(hydro_params)\n",
    "template_hydro_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a12e3e78-29a5-45e2-b33d-28089b9d8a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the computational units\n",
    "for unit in template_hydro_params.keys():\n",
    "    # update the values of parameters in each unit\n",
    "    unit_params = template_hydro_params[unit]\n",
    "    for p in unit_params.keys():\n",
    "        if p in hydrology_dict[unit].keys():\n",
    "            # updating the target gru_entry dictionary\n",
    "            hydrology_dict[unit][p] = unit_params[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f4b1dbd-2d64-4b7a-a948-2ddfe5de26ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'zsnl': '_1ZSNL', 'zpls': 0.109, 'zplg': '_1ZPLG', 'iwf': 1.0},\n",
       " 2: {'zsnl': 0.134, 'zpls': 0.109, 'zplg': 0.312, 'iwf': 1.0},\n",
       " 3: {'zsnl': 0.172, 'zpls': 0.122, 'zplg': 0.223, 'iwf': 1.0},\n",
       " 4: {'zsnl': 0.578, 'zpls': 0.051, 'zplg': 0.13, 'iwf': 1.0},\n",
       " 5: {'zsnl': 0.257, 'zpls': 0.09, 'zplg': 0.26, 'iwf': 1.0},\n",
       " 6: {'zsnl': 0.057, 'zpls': 0.021, 'zplg': 0.02, 'iwf': 1.0},\n",
       " 7: {'zsnl': 0.057, 'zpls': 0.021, 'zplg': 0.02, 'iwf': 1.0},\n",
       " 8: {'zsnl': 0.21, 'zpls': 0.134, 'zplg': 0.134, 'iwf': 1.0},\n",
       " 9: {'zsnl': 0.1, 'zpls': 0.13, 'zplg': 0.13, 'iwf': 1.0},\n",
       " 10: {'zsnl': 0.35, 'zpls': 0.09, 'zplg': 0.26, 'iwf': 1.0},\n",
       " 11: {'zsnl': 0.11, 'zpls': 0.09, 'zplg': 0.26, 'iwf': 0.0},\n",
       " 12: {'zsnl': '_12ZSNL', 'zpls': '_12ZPLS', 'zplg': '_12ZPLG', 'iwf': 1.0}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydrology_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "72543b1f-be36-46f9-bf69-72867ec0800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrology_dict_bounds = {\n",
    "    1: {\n",
    "        'zsnl': [0, 10],\n",
    "        'zplg': [5, 15],\n",
    "       },\n",
    "    12: {\n",
    "        'zpls': [0, 42],\n",
    "        'zsnl': [0, 10],\n",
    "        'zplg': [5, 15],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6cab94-e5e3-4164-b114-2ed5fb4f6151",
   "metadata": {},
   "source": [
    "## Routing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b81cd418-5843-4381-96b1-98a0f27e64f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "routing_params = {\n",
    "    1: ['r2n', 'r1n'],\n",
    "    3: ['pwr', 'flz'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "571626ab-dd1f-42d7-ba10-09c5764c6d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'r2n': '_1R2N', 'r1n': '_1R1N'}, 3: {'pwr': '_3PWR', 'flz': '_3FLZ'}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "routing_template_params = param_list_gen(routing_params)\n",
    "routing_template_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6be2c67d-7409-44fd-9be0-72d124ec71b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the computational units\n",
    "for unit in routing_template_params.keys():\n",
    "    # update the values of parameters in each unit\n",
    "    unit_params = routing_template_params[unit]\n",
    "    for p in unit_params.keys():\n",
    "        if p in routing_dict[unit - 1].keys():\n",
    "            # updating the target gru_entry dictionary\n",
    "            routing_dict[unit - 1][p] = unit_params[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8152f850-0f77-416d-8efb-26e05877115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "routing_dict_bounds = {\n",
    "    1: {\n",
    "        'r2n': [0.0, 1.0],\n",
    "        'r1n': [0.0, 1.0],\n",
    "    },\n",
    "    3: {\n",
    "        'pwr': [0.0, 1.0],\n",
    "        'flz': [0.0, 1.0],\n",
    "    },\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7cb914-34cc-4caf-b23b-eba14ee721cb",
   "metadata": {},
   "source": [
    "## CLASS parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "276577d9-e5f0-4fd2-9be5-e7f543f1b80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already done above, as constrains also needed to be defined right after"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b6b46a-e587-4bf5-8408-518631fd0736",
   "metadata": {},
   "source": [
    "## Calibration file generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "878e5b48-59f6-409d-922a-7cc0304a77cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_config = {\n",
    "    'random_seed': int(time.time()),\n",
    "    'algorithm': 'DDS',\n",
    "    'algorithm_specs': { # use calibration software specific keys\n",
    "        'PerturbationValue': 0.2,\n",
    "        'MaxIteration': 10_000,\n",
    "        'UseRandomParamValue': None,\n",
    "    },\n",
    "    'metric': 'kge_2012',\n",
    "    'parameters': {\n",
    "        'gru_entry': gru_entry,\n",
    "        'hydrology_dict': hydrology_dict,\n",
    "        'routing_dict': routing_dict,\n",
    "    },\n",
    "    'calibration_bounds': {\n",
    "        'gru_entry': gru_entry_bounds,\n",
    "        'hydrology_dict': hydrology_dict_bounds,\n",
    "        'routing_dict': routing_dict_bounds,\n",
    "        # 'routing_dict': None,\n",
    "    },\n",
    "    'constraints': {\n",
    "        'gru_entry': calibration_constraints,\n",
    "        'hydrology_dict': None,\n",
    "        'routing_dict': None,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e3bff722-9445-455d-9f8d-d56dd4f336d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_file = 'ostIn.txt.jinja2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bccc0b14-017b-4f43-9fb4-97b76653c9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'r2n': '_1R2N', 'r1n': '_1R1N', 'pwr': 1.361, 'flz': 4.2e-05},\n",
       " {'r2n': 0.05, 'r1n': 0.119, 'pwr': 1.361, 'flz': 4.2e-05},\n",
       " {'r2n': 0.05, 'r1n': 0.119, 'pwr': '_3PWR', 'flz': '_3FLZ'},\n",
       " {'r2n': 0.05, 'r1n': 0.119, 'pwr': 1.361, 'flz': 4.2e-05},\n",
       " {'r2n': 0.05, 'r1n': 0.119, 'pwr': 1.361, 'flz': 4.2e-05}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "routing_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "970187d7-95e9-4c3a-86a3-2ce0bafabd0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'r2n': [0.0, 1.0], 'r1n': [0.0, 1.0]},\n",
       " 3: {'pwr': [0.0, 1.0], 'flz': [0.0, 1.0]}}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "routing_dict_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e51cbaba-7737-423b-8641-7d2300dbf650",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TemplateNotFound",
     "evalue": "'ostIn.txt.jinja2' not found in search path: '../src/fiatmodel/calibration/ostrich/templates/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTemplateNotFound\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# create the template environment\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m template \u001b[38;5;241m=\u001b[39m \u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# default dictionaries for each calibration software\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdefault_dicts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdefault_dicts\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/virtualenvs/fiatmodel/lib/python3.10/site-packages/jinja2/environment.py:1016\u001b[0m, in \u001b[0;36mEnvironment.get_template\u001b[0;34m(self, name, parent, globals)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1014\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoin_path(name, parent)\n\u001b[0;32m-> 1016\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/virtualenvs/fiatmodel/lib/python3.10/site-packages/jinja2/environment.py:975\u001b[0m, in \u001b[0;36mEnvironment._load_template\u001b[0;34m(self, name, globals)\u001b[0m\n\u001b[1;32m    971\u001b[0m             template\u001b[38;5;241m.\u001b[39mglobals\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mglobals\u001b[39m)\n\u001b[1;32m    973\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m template\n\u001b[0;32m--> 975\u001b[0m template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_globals\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[cache_key] \u001b[38;5;241m=\u001b[39m template\n",
      "File \u001b[0;32m~/Documents/virtualenvs/fiatmodel/lib/python3.10/site-packages/jinja2/loaders.py:126\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[0;34m(self, environment, name, globals)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mglobals\u001b[39m \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# first we try to get the source for this template together\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# with the filename and the uptodate function.\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m source, filename, uptodate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# try to load the code from the bytecode cache if there is a\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# bytecode cache configured.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m bcc \u001b[38;5;241m=\u001b[39m environment\u001b[38;5;241m.\u001b[39mbytecode_cache\n",
      "File \u001b[0;32m~/Documents/virtualenvs/fiatmodel/lib/python3.10/site-packages/jinja2/loaders.py:209\u001b[0m, in \u001b[0;36mFileSystemLoader.get_source\u001b[0;34m(self, environment, template)\u001b[0m\n\u001b[1;32m    207\u001b[0m     plural \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearchpath) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaths\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m     paths_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mrepr\u001b[39m(p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearchpath)\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TemplateNotFound(\n\u001b[1;32m    210\u001b[0m         template,\n\u001b[1;32m    211\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m not found in search \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplural\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpaths_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    212\u001b[0m     )\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    215\u001b[0m     contents \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[0;31mTemplateNotFound\u001b[0m: 'ostIn.txt.jinja2' not found in search path: '../src/fiatmodel/calibration/ostrich/templates/'"
     ]
    }
   ],
   "source": [
    "# create the template environment\n",
    "template = environment.get_template(template_file)\n",
    "\n",
    "# default dictionaries for each calibration software\n",
    "import default_dicts as default_dicts\n",
    "template.globals[\"default_dicts\"] = default_dicts\n",
    "\n",
    "# create content\n",
    "content = template.render(\n",
    "    info=calibration_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bff755-48d6-4b58-b04a-26521e76c529",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiatmodel",
   "language": "python",
   "name": "fiatmodel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
