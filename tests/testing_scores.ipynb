{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a22fdbe-1bbf-483a-94c2-a45e237c1a2e",
   "metadata": {},
   "source": [
    "Testing the output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a015ce65-a9ff-43af-988d-8e759bef22a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import meshflow as mf\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numexpr as ne\n",
    "\n",
    "import HydroErr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb6787d-4854-4e6f-b880-e6eb41b3d954",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Precompile regexes for speed/readability\n",
    "_INT_RE = re.compile(r'^[-+]?\\d+$')\n",
    "_FLOAT_RE = re.compile(\n",
    "    r\"\"\"^[-+]?(                # optional sign\n",
    "        (?:\\d+\\.\\d*|\\d*\\.\\d+)  # something with a decimal point\n",
    "        (?:[eE][-+]?\\d+)?      # optional exponent\n",
    "        |\n",
    "        \\d+[eE][-+]?\\d+        # or integer with exponent (e.g. 1e6)\n",
    "    )$\"\"\",\n",
    "    re.X\n",
    ")\n",
    "\n",
    "# default environment\n",
    "my_env = os.environ.copy()\n",
    "\n",
    "def _parse_numeric_string(s: str):\n",
    "    \"\"\"\n",
    "    Try to interpret a numeric-looking string as int or float.\n",
    "    Return the converted number, or the original string if not numeric.\n",
    "    \"\"\"\n",
    "    if _INT_RE.match(s):\n",
    "        # Keep as int if it fits typical Python int (Python int is unbounded anyway)\n",
    "        return int(s)\n",
    "    if _FLOAT_RE.match(s):\n",
    "        # Anything with decimal point or exponent\n",
    "        return float(s)\n",
    "    return s  # not numeric-looking\n",
    "\n",
    "def _convert_numeric_strings(obj):\n",
    "    \"\"\"\n",
    "    Recursively walk lists/dicts and convert numeric-like strings.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: _convert_numeric_strings(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, list):\n",
    "        return [_convert_numeric_strings(v) for v in obj]\n",
    "    if isinstance(obj, str):\n",
    "        return _parse_numeric_string(obj.strip())\n",
    "    return obj  # leaves int, float, bool, None, etc. untouched\n",
    "\n",
    "def _make_object_hook():\n",
    "    def object_hook(d):\n",
    "        for k, v in d.items():\n",
    "            d[k] = _convert_numeric_strings(v)  # reuse earlier function\n",
    "        return d\n",
    "    return object_hook\n",
    "\n",
    "def _reset_dir(path: str) -> None:\n",
    "    shutil.rmtree(path, ignore_errors=True)  # delete the directory entirely\n",
    "    os.makedirs(path, exist_ok=True)         # recreate it empty\n",
    "\n",
    "def infer_frequency(time_index: pd.DatetimeIndex):\n",
    "    # Try explicit or inferred freq\n",
    "    if time_index.freq is not None:\n",
    "        return time_index.freq\n",
    "    if time_index.inferred_freq is not None:\n",
    "        return pd.tseries.frequencies.to_offset(time_index.inferred_freq)\n",
    "    # Fallback: choose most common delta\n",
    "    if len(time_index) < 2:\n",
    "        raise ValueError(\"Cannot infer frequency from fewer than 2 timestamps.\")\n",
    "    deltas = pd.Series(time_index[1:] - time_index[:-1])\n",
    "    # mode() can return multiple; take the first\n",
    "    step = deltas.mode().iloc[0]\n",
    "    return pd.tseries.frequencies.to_offset(step)\n",
    "\n",
    "def build_calibration_subset(ds: xr.Dataset, dates: dict) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Reindex ds to cover all [start, end] intervals in eval_config['dates'],\n",
    "    padding outside ds.time with NaNs.\n",
    "    \"\"\"\n",
    "    # Extract intervals\n",
    "    starts = pd.to_datetime([d['start'] for d in dates])\n",
    "    ends   = pd.to_datetime([d['end'] for d in dates])\n",
    "    \n",
    "    if len(starts) != len(ends):\n",
    "        raise ValueError(\"Starts and ends length mismatch.\")\n",
    "\n",
    "    # Get underlying pandas index (assumes standard datetime)\n",
    "    try:\n",
    "        time_index = ds.indexes['time']\n",
    "    except KeyError:\n",
    "        raise KeyError(\"Dataset has no 'time' coordinate index.\")\n",
    "\n",
    "    # Infer frequency\n",
    "    freq = infer_frequency(time_index)\n",
    "\n",
    "    # Build union of all desired times\n",
    "    union_index = None\n",
    "    for s, e in zip(starts, ends):\n",
    "        if e < s:\n",
    "            raise ValueError(f\"End before start for interval {s} - {e}\")\n",
    "        rng = pd.date_range(s, e, freq=freq)\n",
    "        union_index = rng if union_index is None else union_index.union(rng)\n",
    "\n",
    "    # Report expansion intent\n",
    "    orig_min, orig_max = time_index.min(), time_index.max()\n",
    "    requested_min, requested_max = union_index.min(), union_index.max()\n",
    "    if requested_min < orig_min or requested_max > orig_max:\n",
    "        raise KeyError(\"Requested calibration range beyond simulation time-series\")\n",
    "\n",
    "    # Reindex (no fill method => NaNs)\n",
    "    out = ds.reindex(time=union_index)\n",
    "    return out\n",
    "\n",
    "def resample_per_variable(ds, rule=\"1D\", dim=\"time\", methods=None, default=None, **kwargs):\n",
    "    \"\"\"\n",
    "    methods: dict var -> reducer (string like 'mean'/'sum' or a callable)\n",
    "    default: reducer for variables not in methods; if None, theyâ€™re skipped\n",
    "    kwargs:  passed to the reducer (e.g., skipna=True, keep_attrs=True)\n",
    "    \"\"\"\n",
    "    if methods is None:\n",
    "        raise ValueError(\"Provide methods, e.g. {'QO': 'sum', 'QI': 'mean'}\")\n",
    "\n",
    "    out = {}\n",
    "    for var in ds.data_vars:\n",
    "        reducer = methods.get(var, default)\n",
    "        if reducer is None:\n",
    "            continue\n",
    "        resampler = ds[var].resample({dim: rule})\n",
    "        if isinstance(reducer, str):\n",
    "            if not hasattr(resampler, reducer):\n",
    "                raise ValueError(f\"Reducer '{reducer}' not available for '{var}'\")\n",
    "            out[var] = getattr(resampler, reducer)(**kwargs)\n",
    "        elif callable(reducer):\n",
    "            out[var] = resampler.reduce(reducer, **kwargs)\n",
    "        else:\n",
    "            raise TypeError(f\"Reducer for '{var}' must be a string or callable\")\n",
    "    return xr.Dataset(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bfa4b1-d635-407c-9852-4987362ef586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defaults\n",
    "with open(os.path.join('defaults.json'), 'r') as f:\n",
    "    DEFAULTS = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eae9a0-7613-43d0-a2ce-1ad7ee24c58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/kasrakeshavarz/Downloads/test/cpu_0/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b1c64f-3b96-4320-898a-4e13f5813f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/kasrakeshavarz/Downloads/test/cpu_0/etc/eval/eval.json\", \"r\") as f:\n",
    "    eval_config = json.load(f, object_hook=_make_object_hook())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3627eaf-9e44-4591-b4f2-3bfced212493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the observation file\n",
    "observations = xr.open_dataset(eval_config['observations_file'])\n",
    "\n",
    "# files to be read\n",
    "root_file_path = os.path.join('./etc', 'eval')\n",
    "# `parameters` JSON files are needed to render templates\n",
    "param_file_paths = {k: os.path.join(root_file_path, v)\n",
    "                    for k, v in (eval_config.get('parameters') or {}).items()}\n",
    "# `others` JSON files are also needed to render templates, but they\n",
    "# do not change during calibration\n",
    "others_file_paths = {k: os.path.join(root_file_path, v)\n",
    "                     for k, v in (eval_config.get('others') or {}).items()}\n",
    "\n",
    "# read the parameter files and generate MESH input parameters\n",
    "mesh_inputs = {}\n",
    "for param_name, file_path in param_file_paths.items():\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        mesh_inputs[param_name] = json.load(f, object_hook=_make_object_hook())\n",
    "# doing the same for the `others` files\n",
    "for other_name, file_path in others_file_paths.items():\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        mesh_inputs[other_name] = json.load(f, object_hook=_make_object_hook())\n",
    "\n",
    "# use meshflow to generate the parameter files\n",
    "# class\n",
    "class_file = mf.utility.render_class_template(\n",
    "    class_case=mesh_inputs['case_entry'],\n",
    "    class_info=mesh_inputs['info_entry'],\n",
    "    class_grus=mesh_inputs['class']\n",
    ")\n",
    "# hydrology\n",
    "hydrology_file = mf.utility.render_hydrology_template(\n",
    "    routing_params=mesh_inputs['routing'],\n",
    "    hydrology_params=mesh_inputs['hydrology'],\n",
    ")\n",
    "# apply changes to the MESH instance\n",
    "with open(os.path.join(eval_config['model_instance_path'], \"MESH_parameters_CLASS.ini\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(class_file)\n",
    "with open(os.path.join(eval_config['model_instance_path'], \"MESH_parameters_hydrology.ini\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(hydrology_file)\n",
    "\n",
    "# first read the time-series of obs/sim for\n",
    "#      each element in the `obs` file\n",
    "simulations = xr.open_dataset('/Users/kasrakeshavarz/Downloads/test/cpu_0/model/results/QO_H_GRD.nc')\n",
    "\n",
    "simulations = simulations.assign_coords({'subbasin': simulations.coords[\"subbasin\"].astype(np.int64)})\n",
    "\n",
    "# as a sanity check, make sure both `subbasin` and `time`\n",
    "# dimensions are available in both datasets\n",
    "for dim in ['subbasin', 'time']:\n",
    "    if dim not in simulations.dims:\n",
    "        raise ValueError(\n",
    "            f'Dimension `{dim}` not found in simulation results.'\n",
    "        )\n",
    "    if dim not in observations.dims:\n",
    "        raise ValueError(\n",
    "            f'Dimension `{dim}` not found in observation data.'\n",
    "        )\n",
    "\n",
    "# selected calibration dates\n",
    "sim_sub = build_calibration_subset(\n",
    "    simulations,\n",
    "    eval_config.get('dates')\n",
    ")\n",
    "obs_sub = build_calibration_subset(\n",
    "    observations,\n",
    "    eval_config.get('dates')\n",
    ")\n",
    "\n",
    "# based on the observation file, understand the time-step\n",
    "# interval of the observations\n",
    "obs_ts = str(np.unique(obs_sub['freq'].values)[0])\n",
    "# and extract the simulation time-step accordingly\n",
    "sim_ts = xr.infer_freq(sim_sub['time'])\n",
    "\n",
    "# if the time-steps are different, perform resampling\n",
    "# FIXME: for now, the variables are averaged. As, the script is set to\n",
    "#        work with streamflow only (simplifying assumption). This will\n",
    "#        be fixed in the future releases.\n",
    "# resampling the time-series matching the observations time-step\n",
    "ts_interval = pd.tseries.frequencies.to_offset\n",
    "# check the variable name in DEFAULTS and see if we should take the\n",
    "# `mean` or `sum` during resampling\n",
    "# Suppose ds has variables QO and QI and a time dimension\n",
    "var = set(sim_sub.variables) - set(DEFAULTS.get('default_variables'))\n",
    "\n",
    "if ts_interval(obs_ts) != ts_interval(sim_ts):\n",
    "    for v in var:\n",
    "        how = 'mean' if v in DEFAULTS['output_variables']['mean'] else 'sum'\n",
    "        sim_sub = resample_per_variable(sim_sub, rule=obs_ts, methods={\"QO\": \"sum\", \"QI\": \"mean\"},)\n",
    "else:\n",
    "    pass # just use obs_sub as is\n",
    "\n",
    "# extract names for the `observations` - can be hard-coded\n",
    "station_ids = obs_sub.subbasin.to_numpy().tolist()\n",
    "station_names = obs_sub.name.to_numpy().tolist()\n",
    "\n",
    "# evaluate each objective function\n",
    "of_values = {}\n",
    "\n",
    "for flux, metrics in eval_config.get('objective_functions').items():\n",
    "    sims = {}\n",
    "    obs = {}\n",
    "    # start populating of_values\n",
    "    of_values[flux] = {}\n",
    "    # assign simulation results for the selected flux\n",
    "    for st in station_ids:\n",
    "        # sims dictionary\n",
    "        sims[obs_sub['name'].sel(subbasin=st).to_numpy().tolist()] = sim_sub[flux].sel(subbasin=st).to_series()\n",
    "        # same for obs dictionary\n",
    "        obs[obs_sub['name'].sel(subbasin=st).to_numpy().tolist()] = obs_sub[flux].sel(subbasin=st).to_series()\n",
    "    # metric (for example, kge_2012), and ofs (list of individual objective functions\n",
    "    for metric, ofs in metrics.items():\n",
    "        # add elements to `of_values`\n",
    "        of_values[flux][metric] = []\n",
    "        # calculate the metric value\n",
    "        he_metric = getattr(HydroErr, metric)\n",
    "        metric_dict = {}\n",
    "        for name in obs.keys():\n",
    "            metric_dict[name] = he_metric(sims[name], obs[name])\n",
    "\n",
    "        for idx, of in enumerate(ofs, start=1): # a list of objective functions\n",
    "            result = ne.evaluate(of, local_dict=metric_dict)\n",
    "            of_values[flux][metric] = result\n",
    "\n",
    "            # write the of results to a .csv file (with only a single element)\n",
    "            with open(\n",
    "                os.path.join(\n",
    "                    './etc',\n",
    "                    'eval',\n",
    "                    f'{flux.upper()}_{metric}_{idx}.csv',\n",
    "                ),\n",
    "                'w',\n",
    "            ) as f:\n",
    "                f.write(f'{result}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31134d0a-dc95-4a03-b413-4cdf30a2b50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "of_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66261ea4-5d9d-4f8b-822e-2e042fd2d47f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiatmodel",
   "language": "python",
   "name": "fiatmodel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
