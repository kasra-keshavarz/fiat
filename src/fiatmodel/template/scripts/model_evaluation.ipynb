{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "\n",
    "from shapely.geometry import Point\n",
    "\n",
    "\n",
    "# built-in libraries\n",
    "from itertools import product\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import sqlite3\n",
    "import datetime\n",
    "\n",
    "# import HydroErr\n",
    "import HydroErr as he"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we calibrate the models based on the `KGE` metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define calibration model\n",
    "# basic instructions\n",
    "basin_name = 'calgary'\n",
    "model = 'mesh'\n",
    "gauges = ['05BH004']\n",
    "\n",
    "# start and end date for the calibration period\n",
    "cal_start = '1985-01-01T00:00:00'\n",
    "cal_end = '2014-12-31T23:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "home = os.getenv('HOME')\n",
    "obs_path = f'{home}/downloads/Hydat.sqlite3'\n",
    "\n",
    "# read observed data\n",
    "conn = sqlite3.connect(obs_path) # locate your HYDAT sqlite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_the_daily_dataframe(\n",
    "    df: pd.DataFrame,\n",
    "    regex_str: str,\n",
    "    col: str,\n",
    "    *args,\n",
    "    **kwargs,\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    # filter and trim columns\n",
    "    df = df.filter(regex=regex_str, axis=1) # extract the columns\n",
    "    df.columns = df.columns.str.replace(r'\\D', '', regex=True) # remove non-digits\n",
    "    df = df.stack(future_stack=True) # stack without dropping\n",
    "    df.index.names = ['STATION_NUMBER', 'YEAR', 'MONTH', 'DAY'] # assign index names\n",
    "    df = df.reset_index() # reset index to add another level\n",
    "    df['DATE'] = pd.to_datetime(df[['YEAR', 'MONTH', 'DAY']].astype(str).agg('-'.join, axis=1), errors='coerce') # define date column\n",
    "    df.drop(columns=['YEAR', 'MONTH', 'DAY'], inplace=True) # drop unnecessary columns\n",
    "    df.dropna(subset=['DATE'], inplace=True) # remove invalid dates\n",
    "    df.set_index(keys=['STATION_NUMBER', 'DATE'], drop=True, inplace=True) # set index levels\n",
    "    df.columns = [col] # assing column name\n",
    "    \n",
    "    # pivot data to look nice\n",
    "    df = df.unstack(level='STATION_NUMBER')\n",
    "    df = df.reorder_levels(order=[1,0], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_station_daily_flow(\n",
    "    station: str,\n",
    "    connection: sqlite3.Connection,\n",
    "    start_date: str = '1850-01-01',\n",
    "    end_date: str = str(datetime.datetime.now().date()),\n",
    "    *args,\n",
    "    **kwargs,\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    '''\n",
    "    This function simply extracts data from the HYDAT sqlite3 database\n",
    "    '''\n",
    "    \n",
    "    # read station data\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM DLY_FLOWS WHERE STATION_NUMBER LIKE '%{station}%'\", connection)\n",
    "    \n",
    "    # set index\n",
    "    df.set_index(keys=['STATION_NUMBER', 'YEAR', 'MONTH'], drop=True, inplace=True)\n",
    "    \n",
    "    # get the FLOW and FLAG\n",
    "    df_flow = get_the_daily_dataframe(df, r'^FLOW\\d', 'FLOW')\n",
    "    df_flag = get_the_daily_dataframe(df, r'^FLOW_.', 'FLAG')\n",
    "    \n",
    "    df = pd.concat([df_flow, df_flag], \n",
    "                   axis=1)\n",
    "    df.sort_index(axis=0, inplace=True)\n",
    "    df.sort_index(axis=1, level=0, ascending=False, inplace=True)\n",
    "    \n",
    "    df = df.loc[start_date:end_date, :]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_station_coords(\n",
    "    station: str,\n",
    "    connection: sqlite3.Connection,\n",
    ") -> dict:\n",
    "    '''\n",
    "    returns `latitude` and `longitude` values for `station`\n",
    "    '''\n",
    "    \n",
    "    # read the specs\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM STATIONS WHERE STATION_NUMBER LIKE '%{station}%'\", conn)\n",
    "    \n",
    "    # rename the columns to their lowercase equivalent\n",
    "    df.rename(\n",
    "        columns={\n",
    "            'LATITUDE': 'latitude',\n",
    "            'LONGITUDE': 'longitude',\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "    \n",
    "    # making a dictionary out of the values\n",
    "    vals_dict = df.loc[:, ['latitude', 'longitude']].to_dict(orient='list')\n",
    "    \n",
    "    # return the values in form of a dictionary\n",
    "    vals_dict = {k:v[0] for k,v in vals_dict.items()}\n",
    "    \n",
    "    return vals_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observation time-series\n",
    "# obs = extract_station_daily_flow(gauges[0], conn, start_date=cal_start, end_date=cal_end)\n",
    "# easier solution\n",
    "obs = pd.read_csv(\n",
    "    './etc/obs.csv',\n",
    "    index_col=[0],\n",
    "    header=[0,1],\n",
    "    parse_dates=True,\n",
    ")\n",
    "\n",
    "# observations gauge locations\n",
    "# obs_location = extract_station_coords(gauges[0], conn)\n",
    "# easier solution\n",
    "obs_location = {'latitude': 51.05, 'longitude': -114.05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model == 'mesh':\n",
    "    # run MESH\n",
    "    # the module related functions need modifications on individual clusters\n",
    "    command = (\n",
    "    'ml restore scimods; '\n",
    "    f'cd ./{model}; '\n",
    "    'mpirun -n 50 ./mpi_sa_mesh;'\n",
    "    )\n",
    "    result = subprocess.run(\n",
    "        command,\n",
    "        capture_output=True,\n",
    "        shell=True\n",
    "    )\n",
    "    # specify the output file\n",
    "    sim_path = f'./{model}/results/QO_PTS-1440M_AVG.csv'\n",
    "\n",
    "    # read the simulated results\n",
    "    try:\n",
    "        sim = pd.read_csv(\n",
    "            sim_path,\n",
    "            header=None,\n",
    "            index_col=0,\n",
    "            parse_dates=True,\n",
    "            date_format='%Y/%m/%d %H:%M:%S.000'\n",
    "        )\n",
    "    except: # if model crashes, so sim_path becomes corrupt\n",
    "        with open(f\"./{model}/results/kge_2012.csv\", \"w\") as file:\n",
    "            # write a terrible metric value \n",
    "            # to indicate model crash\n",
    "            # OSTRICH assumes a minimization problem\n",
    "            file.write(\"2000\\n\")\n",
    "            sys.exit()\n",
    "\n",
    "    # drainage database path\n",
    "    ddb_path = f'./{model}/MESH_drainage_database.nc'\n",
    "    ddb = xr.open_dataset(\n",
    "        ddb_path,\n",
    "    )\n",
    "    \n",
    "    # subbasin geometries path\n",
    "    geoms = gpd.read_file(\n",
    "        f'./{model}/geometries/bcalgary_subbasins.shp'\n",
    "    )\n",
    "    \n",
    "elif model == 'summa':\n",
    "    pass\n",
    "elif model == 'hype':\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding intersection of the gauge location with the subbasins\n",
    "# defining gauge location Point object\n",
    "point = Point(obs_location['longitude'], obs_location['latitude'])\n",
    "\n",
    "# intersecting with the subbasin polygons\n",
    "intersecting_polygons = geoms[geoms.geometry.intersects(point)]\n",
    "\n",
    "# extracting the sub-basin ID (COMID) in this case\n",
    "basin_id = intersecting_polygons.COMID.values[0]\n",
    "\n",
    "# extracting sub-basin rank\n",
    "subbasin_rank = int(ddb['Rank'].sel(subbasin=basin_id).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepareing the `sim' Series\n",
    "sim_df = sim.loc[:, subbasin_rank]\n",
    "sim_df.index.name = 'time'\n",
    "sim_df.name = 'sim'\n",
    "sim_df = sim_df.resample('D').mean()\n",
    "\n",
    "# preparing the `obs' Series\n",
    "idx = pd.IndexSlice\n",
    "obs_df = obs.loc[:, idx[gauges[0], 'FLOW']]\n",
    "obs_df.index.name = 'time'\n",
    "obs_df.name = 'obs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculating metrics of interest\n",
    "for g in gauges:\n",
    "    # define dataframes\n",
    "    sim_df = sim_df.loc[cal_start:cal_end]\n",
    "    obs_df = obs_df.loc[cal_start:cal_end]\n",
    "    \n",
    "    # calculate metrics - minimization problem\n",
    "    try:\n",
    "        kge = -1 * (he.kge_2012(sim_df, obs_df))\n",
    "    except:\n",
    "        with open(f\"./{model}/results/kge_2012.csv\", \"w\") as file:\n",
    "            # write a terrible metric value \n",
    "            # to indicate model crash\n",
    "            # OSTRICH assumes a minimization problem\n",
    "            file.write(\"2000\\n\")\n",
    "            sys.exit()\n",
    "\n",
    "# printing to a file\n",
    "with open(f\"./{model}/results/kge_2012.csv\", \"w\") as file:\n",
    "    file.write(f\"{kge}\\n\")  # Using an f-string to format the float"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scienv",
   "language": "python",
   "name": "scienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
